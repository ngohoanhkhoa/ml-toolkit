{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=App> 2.4.5\n"
     ]
    }
   ],
   "source": [
    "#Create a connection\n",
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('SparkContext is not created!')\n",
    "\n",
    "sc = SparkContext(master = \"local\", appName = \"App\").getOrCreate()\n",
    "print(sc, sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f4ebc0a25c0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all table exist in spark sesion\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls dataset/dataset_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+------------+\n",
      "|daterep|day|month|year|cases|deaths|countriesandterritories|geoid|countryterritorycode|popdata2018|continentexp|\n",
      "+-------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+------------+\n",
      "|   null| 28|    4|2020|  172|     0|            Afghanistan|   AF|                 AFG|   37172386|        Asia|\n",
      "|   null| 27|    4|2020|   68|    10|            Afghanistan|   AF|                 AFG|   37172386|        Asia|\n",
      "|   null| 26|    4|2020|  112|     4|            Afghanistan|   AF|                 AFG|   37172386|        Asia|\n",
      "|   null| 25|    4|2020|   70|     1|            Afghanistan|   AF|                 AFG|   37172386|        Asia|\n",
      "|   null| 24|    4|2020|  105|     2|            Afghanistan|   AF|                 AFG|   37172386|        Asia|\n",
      "+-------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.format('csv').options(header='true', inferSchema='true').load('dataset/dataset_csv/current-data-on-the-geographic-distribution-of-covid-19-cases-worldwide.csv')\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- daterep: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- countriesandterritories: string (nullable = true)\n",
      " |-- geoid: string (nullable = true)\n",
      " |-- countryterritorycode: string (nullable = true)\n",
      " |-- popdata2018: integer (nullable = true)\n",
      " |-- continentexp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='data_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.createOrReplaceTempView('data_temp')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some function usefull:\n",
    "<br>.withColumn(“newColumnName”, formular)\n",
    "<br>.withColumnRenamed(“oldColumnName”, “newColumnName”)\n",
    "<br>.select(“column1”, “column2”, … , “columnt”, formular) \n",
    "<br>.selectExpr(“column1”, “column2”, … , “columnt”, “formularExpr”)\n",
    "<br>.filter(condition)\n",
    "<br>.groupBy(“column1”, “column2”,…,”columnt”).[avg(), min(), max(), sum()]\n",
    "<br>.join(tableName, on = “columnNameJoin”, how = “leftouter”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----------------------+\n",
      "|casesByCountry|deathsByCountry|countriesandterritories|\n",
      "+--------------+---------------+-----------------------+\n",
      "|        988451|          56245|   United_States_of_...|\n",
      "|        199414|          26977|                  Italy|\n",
      "|        128339|          23293|                 France|\n",
      "|        209465|          23190|                  Spain|\n",
      "|        157149|          21092|         United_Kingdom|\n",
      "|         46687|           7207|                Belgium|\n",
      "|        156337|           5913|                Germany|\n",
      "|         91472|           5806|                   Iran|\n",
      "|         83938|           4637|                  China|\n",
      "|         66501|           4543|                 Brazil|\n",
      "|         38245|           4518|            Netherlands|\n",
      "|        112261|           2900|                 Turkey|\n",
      "|         48489|           2707|                 Canada|\n",
      "|         18926|           2274|                 Sweden|\n",
      "|         15529|           1434|                 Mexico|\n",
      "|         29081|           1352|            Switzerland|\n",
      "|         19648|           1102|                Ireland|\n",
      "|         29435|            934|                  India|\n",
      "|         24027|            928|               Portugal|\n",
      "|         87147|            795|                 Russia|\n",
      "+--------------+---------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT SUM(cases) AS casesByCountry, SUM(deaths) AS deathsByCountry, countriesandterritories FROM data_temp GROUP BY countriesandterritories ORDER BY deathsByCountry DESC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Raw data into Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove column\n",
    "dataset = dataset.drop(\"daterep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove null value\n",
    "dataset_nonull = dataset.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_nonull.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline #Tool for big data\n",
    "from pyspark.ml.feature import StringIndexer #String to Index\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"Index\").fit(dataset_nonull) \\\n",
    "            for column in list(set(['countriesandterritories','geoid', 'countryterritorycode', 'continentexp'])) ]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "datasetIndexed = pipeline.fit(dataset_nonull).transform(dataset_nonull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetIndexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler #Transform to Vector\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"day\", \"month\", \"year\", \"cases\", \"popdata2018\", \n",
    "               \"continentexpIndex\", \"countriesandterritoriesIndex\", \"countryterritorycodeIndex\", \"geoidIndex\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "datasetVector = assembler.transform(datasetIndexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scaler = scaler.fit(datasetVector)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "datasetScaled = scalerModel.transform(datasetVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"scaledFeatures\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"deaths\")\n",
    "\n",
    "datasetSelectedFeatures = selector.fit(datasetScaled).transform(datasetScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_final = datasetSelectedFeatures.withColumn(\"label\",dataset_final.deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = dataset_final.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=1, featuresCol='scaledFeatures', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valuesAndPreds = model.transform(test).select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valuesAndPreds = parsedData.map(lambda p: (float(model.predict(p.scaledFeatures)), p.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(model.coefficients))\n",
    "print(\"Intercept: %s\" % str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = model.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valuesAndPreds.show(valuesAndPreds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
